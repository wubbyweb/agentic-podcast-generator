{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Agentic Podcast Generator Interactive\n",
    "\n",
    "Welcome to the **Agentic Podcast Generator** interactive tutorial! This notebook will guide you through understanding and using this intelligent multi-agent system that creates LinkedIn posts and voice dialog scripts from any topic.\n",
    "\n",
    "## What is the Agentic Podcast Generator?\n",
    "\n",
    "The Agentic Podcast Generator is an AI-powered system that uses multiple specialized agents working together to:\n",
    "- Research topics comprehensively\n",
    "- Generate SEO-optimized keywords and hashtags\n",
    "- Create engaging LinkedIn posts\n",
    "- Convert posts into conversational voice scripts\n",
    "\n",
    "The system follows a hierarchical agent architecture where a **Master Agent** orchestrates several **Sub-agents**, each specializing in different tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Step 1: Explore the Project Structure\n",
    "\n",
    "Let's start by examining the project structure to understand how the code is organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/rj/Programs/agentic-podcast-generator\n",
      "\n",
      "Project structure:\n",
      "ðŸ“„ .env\n",
      "ðŸ“„ .env.example\n",
      "ðŸ“ .git/\n",
      "ðŸ“„ .gitignore\n",
      "ðŸ“ .opencode/\n",
      "ðŸ“ .pytest_cache/\n",
      "ðŸ“ .venv/\n",
      "ðŸ“„ README.md\n",
      "ðŸ“ __pycache__/\n",
      "ðŸ“„ agentic_podcast_generator_tutorial.ipynb\n",
      "ðŸ“„ agentic_system.db\n",
      "ðŸ“ agents/\n",
      "   ðŸ“„ agents/__init__.py\n",
      "   ðŸ“ agents/__pycache__/\n",
      "   ðŸ“„ agents/base_agent.py\n",
      "   ðŸ“„ agents/master_agent.py\n",
      "   ðŸ“ agents/sub_agents/\n",
      "ðŸ“ config/\n",
      "   ðŸ“„ config/__init__.py\n",
      "   ðŸ“ config/__pycache__/\n",
      "   ðŸ“„ config/settings.py\n",
      "ðŸ“ database/\n",
      "   ðŸ“„ database/__init__.py\n",
      "   ðŸ“ database/__pycache__/\n",
      "   ðŸ“„ database/connection.py\n",
      "   ðŸ“„ database/models.py\n",
      "ðŸ“„ main.py\n",
      "ðŸ“„ project_structure.md\n",
      "ðŸ“„ pyproject.toml\n",
      "ðŸ“„ requirements.txt\n",
      "ðŸ“„ sequence.md\n",
      "ðŸ“ services/\n",
      "   ðŸ“„ services/__init__.py\n",
      "   ðŸ“ services/__pycache__/\n",
      "   ðŸ“„ services/logger.py\n",
      "   ðŸ“„ services/openrouter_client.py\n",
      "   ðŸ“„ services/search_api.py\n",
      "   ðŸ“„ services/web_scraper.py\n",
      "ðŸ“„ system_architecture.md\n",
      "ðŸ“„ technical_specifications.md\n",
      "ðŸ“„ test_system.py\n",
      "ðŸ“ tests/\n",
      "ðŸ“ utils/\n",
      "   ðŸ“„ utils/__init__.py\n",
      "   ðŸ“ utils/__pycache__/\n",
      "   ðŸ“„ utils/communication.py\n",
      "   ðŸ“„ utils/retry.py\n",
      "   ðŸ“„ utils/validators.py\n",
      "ðŸ“„ uv.lock\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the project structure\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "\n",
    "# List all files and directories\n",
    "print(\"\\nProject structure:\")\n",
    "for item in sorted(current_dir.iterdir()):\n",
    "    if item.is_file():\n",
    "        print(f\"ðŸ“„ {item.name}\")\n",
    "    else:\n",
    "        print(f\"ðŸ“ {item.name}/\")\n",
    "        # Show contents of key directories\n",
    "        if item.name in ['agents', 'config', 'database', 'services', 'utils']:\n",
    "            for subitem in sorted(item.iterdir()):\n",
    "                if subitem.is_file():\n",
    "                    print(f\"   ðŸ“„ {item.name}/{subitem.name}\")\n",
    "                else:\n",
    "                    print(f\"   ðŸ“ {item.name}/{subitem.name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Step 2: Understand the System Architecture\n",
    "\n",
    "The system consists of the following components:\n",
    "\n",
    "### 1. Master Agent (Coordinator)\n",
    "- **Role**: Orchestrates the entire workflow\n",
    "- **Responsibilities**:\n",
    "  - Manages parallel execution of sub-agents\n",
    "  - Handles sequential processing\n",
    "  - Logs all interactions\n",
    "- **Important Note**: Does NOT use GPT-5 for analysis (despite some outdated documentation)\n",
    "\n",
    "### 2. Perplexity AI Research (sonar model)\n",
    "- **Role**: Performs comprehensive research\n",
    "- **Capabilities**:\n",
    "  - Uses Perplexity AI for deep research\n",
    "  - Provides current facts, trends, and insights\n",
    "  - Validates information credibility\n",
    "- **Note**: This replaces the originally planned GPT-5 analysis\n",
    "\n",
    "### 3. Keyword Generator (Gemini 2.0 Flash)\n",
    "- **Role**: Generates SEO-optimized keywords and hashtags\n",
    "- **Model**: google/gemini-2.0-flash-001\n",
    "- **Output**: Keywords, hashtags, and relevance scores\n",
    "\n",
    "### 4. Post Generator (Grok-3 Mini)\n",
    "- **Role**: Creates engaging LinkedIn posts\n",
    "- **Model**: xai/grok-3-mini\n",
    "- **Style**: Casual, professional, and engaging\n",
    "\n",
    "### 5. Voice Dialog Generator (Grok-3 Mini)\n",
    "- **Role**: Converts posts to conversational voice scripts\n",
    "- **Model**: xai/grok-3-mini\n",
    "- **Output**: Natural, podcast-ready dialog\n",
    "\n",
    "### Database Layer\n",
    "- **SQLite Database**: Persistent storage for sessions, logs, and results\n",
    "- **Tables**: sessions, agent_logs, research_results, keywords, generated_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the main entry point\n",
    "print(\"=== MAIN.PY - Entry Point ===\")\n",
    "with open('main.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    # Show the first 50 lines to understand the structure\n",
    "    print(content[:1000] + \"...\")\n",
    "    \n",
    "print(\"\\n=== MASTER AGENT - Core Orchestrator ===\")\n",
    "with open('agents/master_agent.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    # Show the class definition and key methods\n",
    "    lines = content.split('\\n')\n",
    "    for i, line in enumerate(lines[:40]):\n",
    "        print(f\"{i+1:2d}: {line}\")\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Step 3: Understand the Workflow\n",
    "\n",
    "Here's how the system processes a topic:\n",
    "\n",
    "1. **User Input**: Topic provided via command line\n",
    "2. **Research Phase**: Perplexity AI (sonar model) researches the topic\n",
    "3. **Parallel Processing**:\n",
    "   - Keyword Generator creates keywords/hashtags\n",
    "   - Post Generator creates LinkedIn content\n",
    "   - Voice Dialog Generator creates podcast script\n",
    "4. **Output**: Formatted results with all generated content\n",
    "5. **Logging**: All interactions stored in database\n",
    "\n",
    "### Data Flow Diagram\n",
    "```\n",
    "Topic Input\n",
    "    â†“\n",
    "Perplexity AI Research (sonar)\n",
    "    â†“\n",
    "Parallel Execution:\n",
    "â”œâ”€â”€ Keyword Generation (Gemini 2.0 Flash)\n",
    "â”œâ”€â”€ Post Generation (Grok-3 Mini)\n",
    "â””â”€â”€ Voice Script Generation (Grok-3 Mini)\n",
    "    â†“\n",
    "Final Output + Database Logging\n",
    "```\n",
    "\n",
    "### Important Clarification: GPT-5 References\n",
    "\n",
    "**GPT-5 is NOT used anywhere in the actual running application.** Despite some outdated references in documentation and tests, the system uses:\n",
    "- **Perplexity AI (sonar)** for research\n",
    "- **Gemini 2.0 Flash** for keywords\n",
    "- **Grok-3 Mini** for posts and voice scripts\n",
    "\n",
    "The original design planned to use GPT-5 for topic analysis, but the implementation uses Perplexity AI instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the workflow in the master agent\n",
    "import re\n",
    "\n",
    "with open('agents/master_agent.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "# Find the main processing method\n",
    "pattern = r'async def process_topic_with_research.*?return \\{'\n",
    "match = re.search(pattern, content, re.DOTALL)\n",
    "if match:\n",
    "    print(\"=== MAIN PROCESSING WORKFLOW ===\")\n",
    "    workflow_code = match.group(0)\n",
    "    lines = workflow_code.split('\\n')\n",
    "    for i, line in enumerate(lines[:30]):\n",
    "        print(f\"{i+1:2d}: {line}\")\n",
    "    print(\"...\")\n",
    "    \n",
    "# Show parallel execution part\n",
    "parallel_pattern = r'# Execute all sub-agents concurrently.*?return_exceptions=True'\n",
    "match = re.search(parallel_pattern, content, re.DOTALL)\n",
    "if match:\n",
    "    print(\"\\n=== PARALLEL EXECUTION ===\")\n",
    "    print(match.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Step 4: Check Environment Setup\n",
    "\n",
    "Let's check if the environment is properly set up for running the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if required packages are available\n",
    "required_packages = ['asyncio', 'json', 'pathlib', 'dotenv']\n",
    "missing_packages = []\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"âœ“ {package} is available\")\n",
    "    except ImportError:\n",
    "        missing_packages.append(package)\n",
    "        print(f\"âœ— {package} is missing\")\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\nMissing packages: {missing_packages}\")\n",
    "    print(\"Run: pip install -r requirements.txt\")\n",
    "else:\n",
    "    print(\"\\nâœ“ All basic packages are available!\")\n",
    "\n",
    "# Check if .env file exists\n",
    "from pathlib import Path\n",
    "env_file = Path('.env')\n",
    "if env_file.exists():\n",
    "    print(\"âœ“ .env file exists\")\n",
    "else:\n",
    "    print(\"âœ— .env file missing - copy from .env.example\")\n",
    "\n",
    "# Check if database exists\n",
    "db_file = Path('agentic_system.db')\n",
    "if db_file.exists():\n",
    "    print(\"âœ“ Database file exists\")\n",
    "else:\n",
    "    print(\"âœ— Database file missing - will be created on first run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 5: Examine Configuration\n",
    "\n",
    "Let's look at the configuration system to understand how the application is configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the configuration\n",
    "print(\"=== CONFIGURATION SETTINGS ===\")\n",
    "try:\n",
    "    from config.settings import config\n",
    "    print(f\"Log level: {config.log_level}\")\n",
    "    print(f\"Database URL: {config.database_url}\")\n",
    "    print(f\"OpenRouter API Key: {'Set' if config.openrouter_api_key else 'Not set'}\")\n",
    "    \n",
    "    # Show model configurations\n",
    "    print(\"\\nModel configurations:\")\n",
    "    for agent, model_config in config.models.items():\n",
    "        print(f\"  {agent}: {model_config.name}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Configuration error: {e}\")\n",
    "    print(\"Make sure .env file is properly configured\")\n",
    "\n",
    "# Show the .env.example file\n",
    "print(\"\\n=== .ENV.EXAMPLE ===\")\n",
    "try:\n",
    "    with open('.env.example', 'r') as f:\n",
    "        print(f.read())\n",
    "except FileNotFoundError:\n",
    "    print(\"No .env.example file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Step 6: Examine the Agent Classes\n",
    "\n",
    "Let's look at the base agent class and one of the sub-agents to understand the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the base agent class\n",
    "print(\"=== BASE AGENT CLASS ===\")\n",
    "with open('agents/base_agent.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    lines = content.split('\\n')\n",
    "    for i, line in enumerate(lines[:40]):\n",
    "        print(f\"{i+1:2d}: {line}\")\n",
    "    print(\"...\")\n",
    "\n",
    "# Examine a sub-agent\n",
    "print(\"\\n=== KEYWORD GENERATOR AGENT ===\")\n",
    "with open('agents/sub_agents/keyword_generator.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    # Show the execute method\n",
    "    lines = content.split('\\n')\n",
    "    in_execute = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'async def execute' in line:\n",
    "            in_execute = True\n",
    "            print(f\"\\nExecute method (line {i+1}):\")\n",
    "        if in_execute:\n",
    "            print(f\"{i+1:3d}: {line}\")\n",
    "            if line.strip() == '':\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 7: Understand the Database Schema\n",
    "\n",
    "Let's examine the database models to understand how data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the database models\n",
    "print(\"=== DATABASE MODELS ===\")\n",
    "with open('database/models.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "# Show the main model classes\n",
    "in_class = False\n",
    "current_class = \"\"\n",
    "for i, line in enumerate(lines):\n",
    "    if line.startswith('class '):\n",
    "        if current_class:\n",
    "            print(f\"\\n{current_class} class:\")\n",
    "        current_class = line.split('(')[0].replace('class ', '')\n",
    "        in_class = True\n",
    "        print(f\"\\n{current_class} (line {i+1}):\")\n",
    "    elif in_class and line.strip().startswith('__tablename__'):\n",
    "        print(f\"  Table: {line.split('=')[1].strip().strip('\"')}\")\n",
    "    elif in_class and line.strip().startswith('id ='):\n",
    "        print(f\"  Primary key: id\")\n",
    "        break  # Just show the first few lines of each class\n",
    "\n",
    "# Show database initialization\n",
    "print(\"\\n=== DATABASE INITIALIZATION ===\")\n",
    "with open('database/connection.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    lines = content.split('\\n')\n",
    "    for i, line in enumerate(lines[:30]):\n",
    "        print(f\"{i+1:2d}: {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 8: Try Basic Usage\n",
    "\n",
    "Let's try running the system with a simple example. **Note**: This requires a valid OpenRouter API key in your .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we can import the main components\n",
    "print(\"Testing imports...\")\n",
    "try:\n",
    "    from agents.master_agent import MasterAgent\n",
    "    from config.settings import config\n",
    "    print(\"âœ“ Core imports successful\")\n",
    "    \n",
    "    # Check if API key is configured\n",
    "    if config.openrouter_api_key:\n",
    "        print(\"âœ“ API key is configured\")\n",
    "        \n",
    "        # Try a simple test (commented out to avoid API calls)\n",
    "        print(\"\\nTo run the system, use:\")\n",
    "        print(\"python main.py \\\"Your Topic Here\\\"\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âœ— API key not configured\")\n",
    "        print(\"Please set OPENROUTER_API_KEY in your .env file\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Import error: {e}\")\n",
    "    print(\"Make sure dependencies are installed: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 9: Run Tests\n",
    "\n",
    "Let's run the test suite to see if everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tests\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Running tests...\")\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, '-m', 'pytest', 'tests/', '-v'], \n",
    "                          capture_output=True, text=True, timeout=60)\n",
    "    \n",
    "    print(\"STDOUT:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\")\n",
    "        print(result.stderr)\n",
    "        \n",
    "    print(f\"\\nReturn code: {result.returncode}\")\n",
    "    \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"Tests timed out\")\n",
    "except FileNotFoundError:\n",
    "    print(\"pytest not found. Install with: pip install pytest\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running tests: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 10: Examine Database Contents\n",
    "\n",
    "If the database exists, let's examine its contents to see previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine database contents\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "db_path = Path('agentic_system.db')\n",
    "if db_path.exists():\n",
    "    print(\"=== DATABASE CONTENTS ===\")\n",
    "    \n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Show all tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    print(f\"Tables: {[table[0] for table in tables]}\")\n",
    "    \n",
    "    # Show sessions\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM sessions\")\n",
    "    session_count = cursor.fetchone()[0]\n",
    "    print(f\"Total sessions: {session_count}\")\n",
    "    \n",
    "    if session_count > 0:\n",
    "        cursor.execute(\"SELECT id, topic, status, created_at FROM sessions ORDER BY created_at DESC LIMIT 5\")\n",
    "        sessions = cursor.fetchall()\n",
    "        print(\"\\nRecent sessions:\")\n",
    "        for session in sessions:\n",
    "            print(f\"  ID: {session[0]}, Topic: {session[1]}, Status: {session[2]}, Created: {session[3]}\")\n",
    "    \n",
    "    # Show agent logs count\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM agent_logs\")\n",
    "    log_count = cursor.fetchone()[0]\n",
    "    print(f\"\\nTotal agent logs: {log_count}\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "else:\n",
    "    print(\"Database does not exist yet. Run the system to create it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Step 11: Check for Outdated GPT-5 References\n",
    "\n",
    "Let's search for any outdated references to GPT-5 in the codebase to understand what needs to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for GPT-5 references in the codebase\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def search_gpt5_references():\n",
    "    \"\"\"Search for GPT-5 references in the codebase\"\"\"\n",
    "    gpt5_files = []\n",
    "    \n",
    "    # Walk through all files\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        # Skip .git, __pycache__, etc.\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules']]\n",
    "        \n",
    "        for file in files:\n",
    "            if file.endswith(('.py', '.md', '.ipynb', '.txt')):\n",
    "                filepath = Path(root) / file\n",
    "                try:\n",
    "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        if re.search(r'GPT-5|gpt-5', content):\n",
    "                            # Count occurrences\n",
    "                            count = re.findall(r'GPT-5|gpt-5', content)\n",
    "                            gpt5_files.append((str(filepath), len(count)))\n",
    "                except Exception as e:\n",
    "                    pass  # Skip files that can't be read\n",
    "    \n",
    "    return gpt5_files\n",
    "\n",
    "# Search for GPT-5 references\n",
    "gpt5_references = search_gpt5_references()\n",
    "\n",
    "print(\"=== GPT-5 REFERENCES FOUND ===\")\n",
    "if gpt5_references:\n",
    "    print(f\"Found {len(gpt5_references)} files with GPT-5 references:\")\n",
    "    for filepath, count in gpt5_references:\n",
    "        print(f\"  {filepath}: {count} reference(s)\")\n",
    "        \n",
    "    print(\"\\n=== IMPORTANT NOTE ===\")\n",
    "    print(\"GPT-5 is NOT used in the actual running application!\")\n",
    "    print(\"These references are in:\")\n",
    "    print(\"- Documentation files (outdated)\")\n",
    "    print(\"- Test files (expecting old defaults)\")\n",
    "    print(\"- Example code (speculative)\")\n",
    "    \n",
    "    print(\"\\n=== ACTUAL MODELS USED ===\")\n",
    "    print(\"Research: sonar (Perplexity AI)\")\n",
    "    print(\"Keywords: google/gemini-2.0-flash-001\")\n",
    "    print(\"Posts: xai/grok-3-mini\")\n",
    "    print(\"Voice: xai/grok-3-mini\")\n",
    "    \n",
    "else:\n",
    "    print(\"No GPT-5 references found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 12: Examine Services and Utilities\n",
    "\n",
    "Let's look at the supporting services that make the system work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the services\n",
    "print(\"=== OPENROUTER CLIENT ===\")\n",
    "with open('services/openrouter_client.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    lines = content.split('\\n')\n",
    "    for i, line in enumerate(lines[:30]):\n",
    "        print(f\"{i+1:2d}: {line}\")\n",
    "    print(\"...\")\n",
    "\n",
    "print(\"\\n=== LOGGER SERVICE ===\")\n",
    "with open('services/logger.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    lines = content.split('\\n')\n",
    "    for i, line in enumerate(lines[:20]):\n",
    "        print(f\"{i+1:2d}: {line}\")\n",
    "    print(\"...\")\n",
    "\n",
    "print(\"\\n=== UTILITIES ===\")\n",
    "for util_file in ['utils/retry.py', 'utils/communication.py', 'utils/validators.py']:\n",
    "    try:\n",
    "        with open(util_file, 'r') as f:\n",
    "            content = f.read()\n",
    "            lines = content.split('\\n')\n",
    "            print(f\"\\n{util_file}:\")\n",
    "            for i, line in enumerate(lines[:10]):\n",
    "                if line.strip():\n",
    "                    print(f\"  {i+1:2d}: {line}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{util_file} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 13: Advanced Features\n",
    "\n",
    "### Session Management\n",
    "\n",
    "- **Resume Sessions**: Use `--session-id` to continue interrupted workflows\n",
    "- **Session Tracking**: All runs are tracked with unique IDs\n",
    "- **Status Monitoring**: Check session status in database\n",
    "\n",
    "### Parallel Processing\n",
    "\n",
    "- Keyword, Post, and Voice generation run simultaneously\n",
    "- Significant performance improvement over sequential execution\n",
    "- Error handling ensures partial failures don't break the system\n",
    "\n",
    "### Error Handling and Recovery\n",
    "\n",
    "- **Retry Mechanisms**: Automatic retries for API failures\n",
    "- **Fallback Models**: Alternative OpenRouter models if primary fails\n",
    "- **Graceful Degradation**: Continues with partial results\n",
    "- **Comprehensive Logging**: All errors logged with context\n",
    "\n",
    "### API Integration\n",
    "\n",
    "- **OpenRouter API**: Unified access to multiple AI models\n",
    "- **Perplexity AI**: Advanced research capabilities\n",
    "- **Async Operations**: Non-blocking API calls\n",
    "- **Rate Limiting**: Built-in request throttling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how to use the system programmatically\n",
    "print(\"=== PROGRAMMATIC USAGE EXAMPLE ===\")\n",
    "print('''\n",
    "from agents.master_agent import MasterAgent\n",
    "import asyncio\n",
    "\n",
    "async def generate_content():\n",
    "    async with MasterAgent() as master:\n",
    "        result = await master.process_topic(\"AI in Healthcare\")\n",
    "        print(f\"Post: {result['linkedin_post']}\")\n",
    "        print(f\"Keywords: {result['keywords']}\")\n",
    "        print(f\"Voice script: {result['voice_dialog']}\")\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(generate_content())\n",
    "''')\n",
    "\n",
    "# Show CLI usage\n",
    "print(\"\\n=== CLI USAGE EXAMPLES ===\")\n",
    "print(\"# Basic usage\")\n",
    "print(\"python main.py \\\"Artificial Intelligence in Healthcare\\\"\")\n",
    "print()\n",
    "print(\"# With verbose logging\")\n",
    "print(\"python main.py \\\"Machine Learning Trends\\\" --verbose\")\n",
    "print()\n",
    "print(\"# Resume a session\")\n",
    "print(\"python main.py \\\"AI Ethics\\\" --session-id 123\")\n",
    "print()\n",
    "print(\"# JSON output\")\n",
    "print(\"python main.py \\\"Blockchain Technology\\\" --output-format json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ†˜ Step 14: Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **OpenRouter API Key Error**\n",
    "   - Ensure API key is correctly set in `.env`\n",
    "   - Check account has sufficient credits\n",
    "   - Verify key permissions\n",
    "\n",
    "2. **Database Connection Error**\n",
    "   - Ensure write permissions in project directory\n",
    "   - Check SQLite installation\n",
    "   - Verify database file integrity\n",
    "\n",
    "3. **Research Failures**\n",
    "   - Check internet connection\n",
    "   - Verify OpenRouter API access\n",
    "   - Some topics may have limited research data\n",
    "\n",
    "4. **Agent Timeouts**\n",
    "   - Increase timeout settings in config\n",
    "   - Check API rate limits\n",
    "   - Reduce concurrent operations\n",
    "\n",
    "### Debug Mode\n",
    "\n",
    "Run with maximum verbosity:\n",
    "```bash\n",
    "python main.py \"Topic\" --verbose\n",
    "```\n",
    "\n",
    "Check logs:\n",
    "- Console output for real-time debugging\n",
    "- `agentic_system.log` for persistent logs\n",
    "- Database tables for detailed execution history\n",
    "\n",
    "### Getting Help\n",
    "\n",
    "- Check the logs in `agentic_system.log`\n",
    "- Review database agent_logs table\n",
    "- Open issues on GitHub\n",
    "- Check OpenRouter API status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common issues\n",
    "print(\"=== TROUBLESHOOTING CHECKS ===\")\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "version = sys.version_info\n",
    "if version.major >= 3 and version.minor >= 9:\n",
    "    print(\"âœ“ Python version is compatible\")\n",
    "else:\n",
    "    print(f\"âœ— Python version {version.major}.{version.minor} may be too old. Requires 3.9+\")\n",
    "\n",
    "# Check for required files\n",
    "required_files = ['main.py', 'agents/master_agent.py', 'config/settings.py']\n",
    "for file in required_files:\n",
    "    if Path(file).exists():\n",
    "        print(f\"âœ“ {file} exists\")\n",
    "    else:\n",
    "        print(f\"âœ— {file} missing\")\n",
    "\n",
    "# Check imports\n",
    "try:\n",
    "    import asyncio\n",
    "    import dotenv\n",
    "    import click\n",
    "    print(\"âœ“ Core dependencies available\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— Missing dependency: {e}\")\n",
    "    print(\"Run: pip install -r requirements.txt\")\n",
    "\n",
    "# Check environment\n",
    "from pathlib import Path\n",
    "if Path('.env').exists():\n",
    "    print(\"âœ“ .env file exists\")\n",
    "    # Check if it has content\n",
    "    with open('.env', 'r') as f:\n",
    "        content = f.read().strip()\n",
    "        if content:\n",
    "            print(\"âœ“ .env file has content\")\n",
    "        else:\n",
    "            print(\"âœ— .env file is empty\")\n",
    "else:\n",
    "    print(\"âœ— .env file missing (copy from .env.example)\")\n",
    "\n",
    "print(\"\\n=== SYSTEM INFO ===\")\n",
    "print(f\"Platform: {sys.platform}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Current directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Step 15: Code Examples and Customization\n",
    "\n",
    "### Creating a Custom Agent\n",
    "\n",
    "Here's how to create a new agent that extends the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how to create a custom agent\n",
    "print(\"=== CUSTOM AGENT TEMPLATE ===\")\n",
    "custom_agent_code = '''\n",
    "from agents.base_agent import BaseAgent\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CustomAgent(BaseAgent):\n",
    "    \"\"\"Custom agent for specialized tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"custom_agent\", \"custom\")\n",
    "    \n",
    "    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the custom agent logic.\"\"\"\n",
    "        \n",
    "        self.validate_input(input_data, [\"topic\"])\n",
    "        \n",
    "        topic = input_data[\"topic\"]\n",
    "        logger.info(f\"Custom agent processing topic: {topic}\")\n",
    "        \n",
    "        # Your custom logic here\n",
    "        # For example, generate a summary, analysis, etc.\n",
    "        \n",
    "        result = {\n",
    "            \"topic\": topic,\n",
    "            \"custom_output\": f\"Custom analysis of {topic}\",\n",
    "            \"timestamp\": \"2024-01-01T00:00:00Z\"\n",
    "        }\n",
    "        \n",
    "        # Log the result\n",
    "        await self.log_action(\"custom_processing\", \n",
    "                            input_data=input_data, \n",
    "                            output_data=result)\n",
    "        \n",
    "        return result\n",
    "'''\n",
    "\n",
    "print(custom_agent_code)\n",
    "\n",
    "print(\"\\n=== HOW TO INTEGRATE CUSTOM AGENT ===\")\n",
    "integration_steps = '''\n",
    "1. Save the custom agent in agents/sub_agents/custom_agent.py\n",
    "2. Import it in agents/master_agent.py\n",
    "3. Add it to the sub_agents dictionary in initialize_all_agents()\n",
    "4. Update the parallel execution in process_topic_with_research()\n",
    "5. Add configuration in config/settings.py if needed\n",
    "'''\n",
    "print(integration_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Step 16: Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've explored the entire Agentic Podcast Generator system. Here's what you've learned:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Multi-Agent Architecture**: How specialized agents work together\n",
    "- **Research-First Approach**: Perplexity AI provides comprehensive research\n",
    "- **Parallel Processing**: Content generation happens simultaneously\n",
    "- **Database Persistence**: Logging and session management\n",
    "- **Error Resilience**: Graceful handling of failures\n",
    "- **Model Accuracy**: GPT-5 is NOT used despite some outdated references\n",
    "- **Modular Design**: Easy to extend and customize\n",
    "\n",
    "### What You Can Do Next\n",
    "\n",
    "1. **Run the System**: Try `python main.py \"Your Topic\"`\n",
    "2. **Add Custom Agents**: Extend functionality for your needs\n",
    "3. **Modify Configurations**: Adjust models and parameters\n",
    "4. **Monitor Performance**: Check logs and database\n",
    "5. **Contribute**: Improve the system and share your changes\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **README.md**: Complete documentation\n",
    "- **system_architecture.md**: Detailed technical specs\n",
    "- **tests/**: Examples of how components work\n",
    "- **GitHub Issues**: Report bugs and request features\n",
    "\n",
    "### Important Reminder\n",
    "\n",
    "**GPT-5 is not used in this system.** The actual models are:\n",
    "- Research: Perplexity AI (sonar)\n",
    "- Keywords: Gemini 2.0 Flash\n",
    "- Posts & Voice: Grok-3 Mini\n",
    "\n",
    "Happy coding! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=== TUTORIAL SUMMARY ===\")\n",
    "print(\"You have successfully explored:\")\n",
    "print(\"âœ“ Project structure and organization\")\n",
    "print(\"âœ“ System architecture and workflow\")\n",
    "print(\"âœ“ Configuration and setup\")\n",
    "print(\"âœ“ Agent classes and execution\")\n",
    "print(\"âœ“ Database schema and models\")\n",
    "print(\"âœ“ Testing and debugging\")\n",
    "print(\"âœ“ Customization and extension\")\n",
    "print(\"âœ“ GPT-5 reference identification\")\n",
    "print()\n",
    "print(\"The Agentic Podcast Generator is ready for your topics!\")\n",
    "print(\"Next: python main.py \\\"Your Favorite Topic\\\"\")\n",
    "print()\n",
    "print(\"Remember: GPT-5 is NOT used - the system uses Perplexity AI, Gemini, and Grok models!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-podcast-generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
